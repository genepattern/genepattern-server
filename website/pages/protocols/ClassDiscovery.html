<!--
  ~ Copyright 2012 The Broad Institute, Inc.
  ~ SOFTWARE COPYRIGHT NOTICE
  ~ This software and its documentation are the copyright of the Broad Institute, Inc. All rights are reserved.
  ~
  ~ This software is supplied without any warranty or guaranteed support whatsoever. The Broad Institute is not responsible for its use, misuse, or functionality.
  -->

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
        <META HTTP-EQUIV="CACHE-CONTROL" CONTENT="NO-CACHE" />
        <title>Clustering and Class Discovery</title>
        <link href="../../css/protocolsWindow.css" rel="stylesheet" type="text/css" />
        <script type="text/javascript" src="../../js/protocol.js"></script>
    </head>

    <body>

        <h1>Clustering and Class Discovery</h1>
        <table width="100%" border="0" cellpadding="0" cellspacing="0">
            <tr>
                <td align="left">
                    <a href="protocols.html">protocols</a>
                </td>
            </tr>
        </table>

        <p>Find the innate structure of gene expression data by clustering genes and/or samples.</p>

        <div class="steps">
            <h2>Click the desired algorithm</h2>


            <ul>
                <li><b><a href="ClassDiscovery_hier.html">Hierarchical clustering</a></b> (Eisen et al., 1998) groups elements based on
                    how close they are to one another. The result is a tree structure, referred to as dendrogram. This is a common and valuable approach; however, it is highly sensitive to the measurement used to
                    assess distance and requires you to define clusters subjectively based on the dendogram (Brunet et al., 2004).
                </li>
                <li><b><a href="ClassDiscovery_kmeans.html">K-means clustering</a></b> (MacQueen, 1967) groups elements into a specified number of clusters, which can be useful when you know or suspect the
                    number of clusters in the data. The algorithm randomly selects a
                    center data point for <i>k</i> clusters and assigns each data point to the nearest
                    cluster center. Iteratively, it recalculates a new center data point for each cluster based on the mean value of its members and reassigns all data
                    points to the closest cluster center until the
                    distance between consecutive cluster centers converges into <i>k</i> stable clusters.
                </li>
                <li><b><a href="ClassDiscovery_nmf.html">Non-negative matrix factorization (NMF)</a></b> (Brunet et al., 2004) clusters the data by
                    breaking it down into metagenes or metasamples,
                    each of which represents a group of genes or samples, respectively. NMF extracts features that may more accurately correspond to biological processes.
                </li>
            </ul>
        </div>


        <h2>References</h2>

        <p>Brunet, J-P., Tamayo, P., Golub, T.R., and Mesirov, J.P. 2004. Metagenes and molecular pattern discovery using
            matrix factorization. <i>Proc. Natl. Acad. Sci. USA</i> 101(12):4164–4169.</p>

        <p>Eisen, M.B., Spellman, P.T., Brown, P.O., and Botstein, D. 1998. Cluster Analysis and Display of Genome-Wide Expression Patterns. <i>Proc. Natl. Acad. Sci. USA</i> 95:14863-14868.</p>

        <p>MacQueen, J. B. 1967. Some Methods for classification and Analysis of Multivariate Observations. <i>In</i> Proceedings of Fifth Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1. University of California Press, California. pp. 281-297.</p>

    </body>
</html>
